{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70fddac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from hashlib import md5\n",
    "from collections import Counter\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.stats import zscore\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68b3be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "OUTPUT_DIR = \"data_cleaned\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79fdf219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Helper Functions ----------\n",
    "def parse_and_clean_cats(cat_field):\n",
    "    if not isinstance(cat_field, str):\n",
    "        return []\n",
    "    raw = [c.strip() for c in cat_field.split(\",\") if c.strip()]\n",
    "    out = []\n",
    "    for c in raw:\n",
    "        c2 = c.lower()\n",
    "        if c2.endswith(\" food\"):\n",
    "            c2 = c2[: -len(\" food\")]\n",
    "        out.append(c2)\n",
    "    return out\n",
    "\n",
    "def valid_hours(day_hours):\n",
    "    try:\n",
    "        start, end = day_hours.split(\"-\")\n",
    "        return int(start.replace(\":\", \"\")) < int(end.replace(\":\", \"\"))\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def valid_hours_dict(h):\n",
    "    if not isinstance(h, dict):\n",
    "        return False\n",
    "    return all(isinstance(v, str) and valid_hours(v) for v in h.values())\n",
    "\n",
    "def parse_dates(s):\n",
    "    out = []\n",
    "    for part in s.split(\",\"):\n",
    "        try:\n",
    "            out.append(datetime.strptime(part.strip(), \"%Y-%m-%d %H:%M:%S\"))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return sorted(set(out))\n",
    "\n",
    "def sentiment_ok(row, analyzer):\n",
    "    s = analyzer.polarity_scores(row.text)[\"compound\"]\n",
    "    if row.stars >= 4 and s < 0:   return False\n",
    "    if row.stars <= 2 and s > 0.5: return False\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "546f7f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning business.json...\n",
      "business_clean.csv saved\n"
     ]
    }
   ],
   "source": [
    "# ---------- 1) Business ----------\n",
    "print(\"Cleaning business.json...\")\n",
    "df = pd.read_json(\"Data/business.json\", lines=True)\n",
    "# Drop incomplete\n",
    "mandatory = [\"business_id\",\"name\",\"city\",\"state\",\"latitude\",\"longitude\",\"stars\",\"review_count\"]\n",
    "df.dropna(subset=mandatory, inplace=True)\n",
    "# Unique IDs by review_count\n",
    "df.sort_values(\"review_count\", ascending=False, inplace=True)\n",
    "df.drop_duplicates(\"business_id\", keep=\"first\", inplace=True)\n",
    "# Filter review_count >=3\n",
    "df = df[df.review_count >= 3]\n",
    "# Stars bounds & rounding\n",
    "df = df[df.stars.between(0,5)]\n",
    "df[\"stars\"] = (df.stars * 2).round() / 2.0\n",
    "# Hours\n",
    "df = df[df.hours.apply(valid_hours_dict)]\n",
    "# Numeric outliers\n",
    "for col in [\"review_count\",\"stars\"]:\n",
    "    z = (df[col] - df[col].mean())/df[col].std()\n",
    "    df = df[np.abs(z) <= 3]\n",
    "# Categories\n",
    "df[\"categories\"] = df.categories.apply(parse_and_clean_cats)\n",
    "def clean_cats(cat_list): return [c.strip().lower().replace(\"food\",\"\") for c in cat_list]\n",
    "df[\"categories\"] = df.categories.apply(clean_cats)\n",
    "# Write\n",
    "business_out = df.reset_index(drop=True)\n",
    "business_out.to_csv(os.path.join(OUTPUT_DIR, \"business_clean.csv\"), index=False)\n",
    "print(\"business_clean.csv saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80d609c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning checkin.json...\n",
      "checkin_clean.csv saved\n"
     ]
    }
   ],
   "source": [
    "# ---------- 2) Check-in ----------\n",
    "print(\"Cleaning checkin.json...\")\n",
    "raw = pd.read_json(\"Data/checkin.json\", lines=True)\n",
    "raw = raw.groupby(\"business_id\").agg({\"date\": lambda L: \",\".join(L)}).reset_index()\n",
    "raw[\"checkins\"] = raw.date.apply(parse_dates)\n",
    "raw = raw[raw.checkins.str.len() > 0]\n",
    "rows = []\n",
    "for _, r in raw.iterrows():\n",
    "    for ts in r.checkins:\n",
    "        rows.append({\"business_id\": r.business_id, \"checkin_time\": ts})\n",
    "events = pd.DataFrame(rows)\n",
    "# Filter >=5 per business\n",
    "agg = events.groupby(\"business_id\").checkin_time.count().rename(\"n_checkins\")\n",
    "good = agg[agg >= 5].index\n",
    "events = events[events.business_id.isin(good)]\n",
    "# Write\n",
    "events.to_csv(os.path.join(OUTPUT_DIR, \"checkin_clean.csv\"), index=False)\n",
    "print(\"checkin_clean.csv saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa495815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning review.json (may take a while)...\n",
      "review_clean.csv saved.\n"
     ]
    }
   ],
   "source": [
    "# ---------- 3) Reviews ----------\n",
    "print(\"Cleaning review.json (may take a while)...\")\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "chunksize = 100_000\n",
    "first = True\n",
    "for chunk in pd.read_json(\"Data/review.json\", lines=True, chunksize=chunksize):\n",
    "    # mandatory + dedupe\n",
    "    mandatory = [\"review_id\",\"user_id\",\"business_id\",\"stars\",\"date\",\"text\"]\n",
    "    chunk.dropna(subset=mandatory, inplace=True)\n",
    "    chunk.drop_duplicates(\"review_id\", keep=\"first\", inplace=True)\n",
    "    # date parse & range\n",
    "    chunk[\"date_parsed\"] = pd.to_datetime(chunk.date, format=\"%Y-%m-%d\", errors='coerce')\n",
    "    chunk = chunk[chunk.date_parsed.between(\"2010-01-01\",\"2020-12-31\")]\n",
    "    # stars & text length\n",
    "    chunk = chunk[chunk.stars.between(1,5)]\n",
    "    chunk = chunk[chunk.text.str.len() >= 5]\n",
    "    # text hash dedupe\n",
    "    chunk[\"text_hash\"] = chunk.text.map(lambda t: md5(t.encode('utf8')).hexdigest())\n",
    "    chunk.drop_duplicates(\"text_hash\", keep=\"first\", inplace=True)\n",
    "    # vote counts\n",
    "    for col in [\"useful\",\"funny\",\"cool\"]:\n",
    "        chunk = chunk[chunk[col] >= 0]\n",
    "    # z-score outliers\n",
    "    for col in [\"useful\",\"funny\",\"cool\"]:\n",
    "        zs = zscore(chunk[col])\n",
    "        chunk = chunk[zs < 3]\n",
    "    # sentiment mismatch\n",
    "    chunk = chunk[chunk.apply(lambda r: sentiment_ok(r, analyzer), axis=1)]\n",
    "    # drop helper\n",
    "    chunk.drop(columns=[\"text_hash\",\"date_parsed\"], inplace=True)\n",
    "    # write\n",
    "    mode = 'w' if first else 'a'\n",
    "    header = first\n",
    "    chunk.to_csv(os.path.join(OUTPUT_DIR, \"review_clean.csv\"), mode=mode, header=header, index=False)\n",
    "    first = False\n",
    "print(\"review_clean.csv saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "404a5ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning tip.json...\n",
      "tips_clean.csv saved\n"
     ]
    }
   ],
   "source": [
    "# ---------- 4) Tips ----------\n",
    "print(\"Cleaning tip.json...\")\n",
    "df_t = pd.read_json(\"Data/tip.json\", lines=True)\n",
    "# drop missing\n",
    "mandatory = [\"text\",\"date\",\"compliment_count\",\"business_id\",\"user_id\"]\n",
    "df_t.dropna(subset=mandatory, inplace=True)\n",
    "# parse date & filter\n",
    "df_t[\"date_parsed\"] = pd.to_datetime(df_t.date, format=\"%Y-%m-%d\", errors='coerce')\n",
    "mask = (df_t.date_parsed >= \"2010-01-01\") & (df_t.date_parsed <= \"2020-12-31\")\n",
    "df_t = df_t[mask]\n",
    "# compliments & outliers\n",
    "df_t = df_t[df_t.compliment_count >= 0]\n",
    "z_scores = zscore(df_t.compliment_count.astype(float), nan_policy='omit')\n",
    "if not df_t.empty:\n",
    "    df_t = df_t[np.abs(z_scores) < 3]\n",
    "# text length & dedupe \n",
    "df_t[\"text_len\"] = df_t.text.str.len().fillna(0)\n",
    "df_t = df_t[df_t.text_len >= 5]\n",
    "df_t[\"text_hash\"] = df_t.text.map(lambda t: md5(t.encode('utf-8')).hexdigest())\n",
    "df_t.drop_duplicates(subset=[\"text_hash\"], inplace=True)\n",
    "# drop helpers and write\n",
    "cols = [c for c in df_t.columns if c not in [\"text_hash\",\"text_len\",\"date_parsed\"]]\n",
    "df_t[cols].to_csv(os.path.join(OUTPUT_DIR, \"tips_clean.csv\"), index=False)\n",
    "print(\"tips_clean.csv saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16062270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning user.json in chunks...\n",
      "Chunk: wrote 82748 users\n",
      "Chunk: wrote 95253 users\n",
      "Chunk: wrote 92513 users\n",
      "Chunk: wrote 92748 users\n",
      "Chunk: wrote 90169 users\n",
      "Chunk: wrote 92816 users\n",
      "Chunk: wrote 92662 users\n",
      "Chunk: wrote 90450 users\n",
      "Chunk: wrote 93594 users\n",
      "Chunk: wrote 90633 users\n",
      "Chunk: wrote 93601 users\n",
      "Chunk: wrote 90748 users\n",
      "Chunk: wrote 94419 users\n",
      "Chunk: wrote 90868 users\n",
      "Chunk: wrote 95075 users\n",
      "Chunk: wrote 88095 users\n",
      "Chunk: wrote 94758 users\n",
      "Chunk: wrote 89777 users\n",
      "Chunk: wrote 95235 users\n",
      "Chunk: wrote 80557 users\n",
      "Users cleaned.\n"
     ]
    }
   ],
   "source": [
    "# ----------- 5) Users (Chunked) -----------\n",
    "print(\"Cleaning user.json in chunks...\")\n",
    "chunk_size = 100000\n",
    "first = True\n",
    "for chunk in pd.read_json(\"Data/user.json\", lines=True, chunksize=chunk_size):\n",
    "    # 1) mandatory\n",
    "    mand_u = [\"user_id\",\"review_count\",\"yelping_since\",\"useful\",\"funny\",\"cool\",\"fans\",\"average_stars\"]\n",
    "    chunk = chunk.dropna(subset=mand_u)\n",
    "    # 2) parse date\n",
    "    chunk['joined'] = pd.to_datetime(chunk.yelping_since, errors='coerce')\n",
    "    chunk = chunk[chunk.joined.between('2010-01-01','2020-12-31')]\n",
    "    # 3) non-neg\n",
    "    for col in ['review_count','useful','funny','cool','fans','average_stars']:\n",
    "        chunk = chunk[chunk[col] >= 0]\n",
    "    # 4) dedupe\n",
    "    chunk = chunk.drop_duplicates('user_id')\n",
    "    # 5) drop helpers\n",
    "    chunk = chunk.drop(columns=['yelping_since','joined'])\n",
    "    # 6) write\n",
    "    mode = 'w' if first else 'a'\n",
    "    header = first\n",
    "    chunk.to_csv(os.path.join(OUTPUT_DIR, \"user_clean.csv\"), mode=mode, header=header, index=False)\n",
    "    print(f\"Chunk: wrote {len(chunk)} users\")\n",
    "    first = False\n",
    "print(\"Users cleaned.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
