{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639d0e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from hashlib import md5\n",
    "from collections import Counter\n",
    "from scipy.stats import zscore\n",
    "from langdetect import detect\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "374de42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>attributes</th>\n",
       "      <th>categories</th>\n",
       "      <th>hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GEuzwtdp2DX9c9HyhOT9KQ</td>\n",
       "      <td>The Landing Restaurant</td>\n",
       "      <td>22 N Main St</td>\n",
       "      <td>New Hope</td>\n",
       "      <td>PA</td>\n",
       "      <td>18938</td>\n",
       "      <td>40.365223</td>\n",
       "      <td>-74.951180</td>\n",
       "      <td>3.0</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>{'OutdoorSeating': 'True', 'RestaurantsReserva...</td>\n",
       "      <td>[american (new), restaurants]</td>\n",
       "      <td>{'Monday': '11:0-22:0', 'Tuesday': '11:0-22:0'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a62d7e_xXeljJcOUHkJjkg</td>\n",
       "      <td>Circles Contemporary Asian</td>\n",
       "      <td>1516 Tasker St</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>PA</td>\n",
       "      <td>19145</td>\n",
       "      <td>39.930647</td>\n",
       "      <td>-75.170729</td>\n",
       "      <td>3.5</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>{'RestaurantsDelivery': 'True', 'RestaurantsAt...</td>\n",
       "      <td>[vegetarian, asian fusion, thai, restaurants]</td>\n",
       "      <td>{'Monday': '17:0-22:45', 'Tuesday': '11:0-22:4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56TulnfOZy0JI96zwVv27w</td>\n",
       "      <td>Gioia's Deli</td>\n",
       "      <td>1934 Macklind Ave</td>\n",
       "      <td>Saint Louis</td>\n",
       "      <td>MO</td>\n",
       "      <td>63110</td>\n",
       "      <td>38.617402</td>\n",
       "      <td>-90.276806</td>\n",
       "      <td>4.5</td>\n",
       "      <td>376</td>\n",
       "      <td>1</td>\n",
       "      <td>{'BikeParking': 'True', 'RestaurantsAttire': '...</td>\n",
       "      <td>[sandwiches, delis, italian, restaurants]</td>\n",
       "      <td>{'Monday': '10:0-16:0', 'Tuesday': '10:0-16:0'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140WNflw_n01GVGtB-PwJQ</td>\n",
       "      <td>Tarpon Turtle Grill &amp; Marina</td>\n",
       "      <td>1513 Lake Tarpon Ave</td>\n",
       "      <td>Tarpon Springs</td>\n",
       "      <td>FL</td>\n",
       "      <td>34689</td>\n",
       "      <td>28.146169</td>\n",
       "      <td>-82.728302</td>\n",
       "      <td>3.5</td>\n",
       "      <td>376</td>\n",
       "      <td>1</td>\n",
       "      <td>{'BusinessParking': '{'garage': False, 'street...</td>\n",
       "      <td>[american (new), restaurants, sea, caribbean]</td>\n",
       "      <td>{'Monday': '11:0-21:0', 'Tuesday': '11:0-21:0'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4klUc-NroGEM1vGzp2-0Qg</td>\n",
       "      <td>Rudie's Seafood &amp; Sausage</td>\n",
       "      <td>1402 McGavock Pike</td>\n",
       "      <td>Nashville</td>\n",
       "      <td>TN</td>\n",
       "      <td>37216</td>\n",
       "      <td>36.204719</td>\n",
       "      <td>-86.723909</td>\n",
       "      <td>4.5</td>\n",
       "      <td>376</td>\n",
       "      <td>0</td>\n",
       "      <td>{'BikeParking': 'True', 'RestaurantsTakeOut': ...</td>\n",
       "      <td>[sea, restaurants, american (new), cajun/creole]</td>\n",
       "      <td>{'Monday': '16:0-22:0', 'Tuesday': '16:0-22:0'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                          name               address  \\\n",
       "1  GEuzwtdp2DX9c9HyhOT9KQ        The Landing Restaurant          22 N Main St   \n",
       "2  a62d7e_xXeljJcOUHkJjkg    Circles Contemporary Asian        1516 Tasker St   \n",
       "3  56TulnfOZy0JI96zwVv27w                  Gioia's Deli     1934 Macklind Ave   \n",
       "4  140WNflw_n01GVGtB-PwJQ  Tarpon Turtle Grill & Marina  1513 Lake Tarpon Ave   \n",
       "5  4klUc-NroGEM1vGzp2-0Qg     Rudie's Seafood & Sausage    1402 McGavock Pike   \n",
       "\n",
       "             city state postal_code   latitude  longitude  stars  \\\n",
       "1        New Hope    PA       18938  40.365223 -74.951180    3.0   \n",
       "2    Philadelphia    PA       19145  39.930647 -75.170729    3.5   \n",
       "3     Saint Louis    MO       63110  38.617402 -90.276806    4.5   \n",
       "4  Tarpon Springs    FL       34689  28.146169 -82.728302    3.5   \n",
       "5       Nashville    TN       37216  36.204719 -86.723909    4.5   \n",
       "\n",
       "   review_count  is_open                                         attributes  \\\n",
       "1           377        1  {'OutdoorSeating': 'True', 'RestaurantsReserva...   \n",
       "2           377        1  {'RestaurantsDelivery': 'True', 'RestaurantsAt...   \n",
       "3           376        1  {'BikeParking': 'True', 'RestaurantsAttire': '...   \n",
       "4           376        1  {'BusinessParking': '{'garage': False, 'street...   \n",
       "5           376        0  {'BikeParking': 'True', 'RestaurantsTakeOut': ...   \n",
       "\n",
       "                                         categories  \\\n",
       "1                     [american (new), restaurants]   \n",
       "2     [vegetarian, asian fusion, thai, restaurants]   \n",
       "3         [sandwiches, delis, italian, restaurants]   \n",
       "4     [american (new), restaurants, sea, caribbean]   \n",
       "5  [sea, restaurants, american (new), cajun/creole]   \n",
       "\n",
       "                                               hours  \n",
       "1  {'Monday': '11:0-22:0', 'Tuesday': '11:0-22:0'...  \n",
       "2  {'Monday': '17:0-22:45', 'Tuesday': '11:0-22:4...  \n",
       "3  {'Monday': '10:0-16:0', 'Tuesday': '10:0-16:0'...  \n",
       "4  {'Monday': '11:0-21:0', 'Tuesday': '11:0-21:0'...  \n",
       "5  {'Monday': '16:0-22:0', 'Tuesday': '16:0-22:0'...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# business.json\n",
    "# Helpers\n",
    "def parse_and_clean_cats(cat_field):\n",
    "    \"\"\"\n",
    "    - If cat_field is a str, split on commas and strip each piece.\n",
    "    - Otherwise return [].\n",
    "    - Then lowercase & remove trailing “ food” if desired.\n",
    "    \"\"\"\n",
    "    if not isinstance(cat_field, str):\n",
    "        return []\n",
    "    # split & strip\n",
    "    raw = [c.strip() for c in cat_field.split(\",\") if c.strip()]\n",
    "    # normalize\n",
    "    out = []\n",
    "    for c in raw:\n",
    "        c2 = c.lower()\n",
    "        # e.g. unify \"mexican food\" -> \"mexican\"\n",
    "        if c2.endswith(\" food\"):\n",
    "            c2 = c2[: -len(\" food\")]\n",
    "        out.append(c2)\n",
    "    return out\n",
    "\n",
    "# 1. Load\n",
    "df = pd.read_json(\"Data/business.json\", lines=True)\n",
    "\n",
    "# 2. Drop incomplete\n",
    "mandatory = [\"business_id\",\"name\",\"city\",\"state\",\"latitude\",\"longitude\",\"stars\",\"review_count\"]\n",
    "df.dropna(subset=mandatory, inplace=True)\n",
    "\n",
    "# 3. Unique IDs\n",
    "df.sort_values(\"review_count\", ascending=False, inplace=True)\n",
    "df = df.drop_duplicates(\"business_id\", keep=\"first\")\n",
    "\n",
    "# 4. Geographic box\n",
    "'''\n",
    "lat_ok = df.latitude.between(32, 42)\n",
    "lon_ok = df.longitude.between(-125, -114)\n",
    "df = df[lat_ok & lon_ok]\n",
    "'''\n",
    "\n",
    "# 5. DBSCAN for geo‐outliers\n",
    "'''\n",
    "coords = df[[\"latitude\",\"longitude\"]].to_numpy()\n",
    "clust = DBSCAN(eps=0.1, min_samples=5).fit(coords)\n",
    "df = df[clust.labels_ != -1]\n",
    "'''\n",
    "\n",
    "# 6. is_open & review_count\n",
    "# df = df[df.is_open == 1]\n",
    "df = df[df.review_count >= 3]\n",
    "\n",
    "# 7. Stars bounds & rounding\n",
    "df = df[df.stars.between(0, 5)]\n",
    "df[\"stars\"] = (df.stars * 2).round() / 2.0\n",
    "\n",
    "# 8. Hours validation\n",
    "def valid_hours(day_hours):\n",
    "    try:\n",
    "        start,end = day_hours.split(\"-\")\n",
    "        return int(start.replace(\":\", \"\")) < int(end.replace(\":\", \"\"))\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def valid_hours_dict(h):\n",
    "    \"\"\"\n",
    "    Returns True if h is a dict and every value string parses to start<end.\n",
    "    \"\"\"\n",
    "    if not isinstance(h, dict):\n",
    "        return False\n",
    "    # all() over its values (e.g. \"10:00-21:00\")\n",
    "    return all(\n",
    "        isinstance(v, str) and valid_hours(v)\n",
    "        for v in h.values()\n",
    "    )\n",
    "\n",
    "# Now filter:\n",
    "df = df[df.hours.apply(valid_hours_dict)]\n",
    "\n",
    "df = df[df.hours.apply(lambda h: all(valid_hours(v) for v in h.values()))]\n",
    "\n",
    "# 9. Outlier numeric\n",
    "for col in [\"review_count\",\"stars\"]:\n",
    "    z = (df[col] - df[col].mean())/df[col].std()\n",
    "    df = df[np.abs(z) <= 3]\n",
    "\n",
    "# 10. Normalize categories\n",
    "\n",
    "# normalize\n",
    "df[\"categories\"] = df.categories.apply(parse_and_clean_cats)\n",
    "def clean_cats(cat_list):\n",
    "    return [c.strip().lower().replace(\"food\",\"\") for c in cat_list]\n",
    "df[\"categories\"] = df.categories.apply(clean_cats)\n",
    "\n",
    "\n",
    "# Final clean subset\n",
    "clean_df = df.reset_index(drop=True)\n",
    "clean_df.index += 1\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7e70657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>checkin_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>---kPU91CF4Lq2-WlRu9Lw</td>\n",
       "      <td>2020-03-13 21:10:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>---kPU91CF4Lq2-WlRu9Lw</td>\n",
       "      <td>2020-06-02 22:18:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>---kPU91CF4Lq2-WlRu9Lw</td>\n",
       "      <td>2020-07-24 22:42:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>---kPU91CF4Lq2-WlRu9Lw</td>\n",
       "      <td>2020-10-24 21:36:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>---kPU91CF4Lq2-WlRu9Lw</td>\n",
       "      <td>2020-12-09 21:23:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        checkin_time\n",
       "1  ---kPU91CF4Lq2-WlRu9Lw 2020-03-13 21:10:56\n",
       "2  ---kPU91CF4Lq2-WlRu9Lw 2020-06-02 22:18:06\n",
       "3  ---kPU91CF4Lq2-WlRu9Lw 2020-07-24 22:42:27\n",
       "4  ---kPU91CF4Lq2-WlRu9Lw 2020-10-24 21:36:13\n",
       "5  ---kPU91CF4Lq2-WlRu9Lw 2020-12-09 21:23:33"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkin.json\n",
    "\n",
    "# 1. Load & split/merge any duplicate business rows\n",
    "raw = pd.read_json(\"Data/checkin.json\", lines=True)\n",
    "# If there are multiple rows per business, merge:\n",
    "raw = raw.groupby(\"business_id\").agg({\"date\": lambda L: \",\".join(L)}).reset_index()\n",
    "\n",
    "# 2. Parse dates\n",
    "def parse_dates(s):\n",
    "    out = []\n",
    "    for part in s.split(\",\"):\n",
    "        try:\n",
    "            out.append(datetime.strptime(part.strip(), \"%Y-%m-%d %H:%M:%S\"))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return sorted(set(out))  # dedupe & sort\n",
    "\n",
    "raw[\"checkins\"] = raw.date.apply(parse_dates)\n",
    "raw = raw[raw.checkins.str.len() > 0]  # drop empty\n",
    "\n",
    "# 3. Expand to one row per event\n",
    "rows = []\n",
    "for _, r in raw.iterrows():\n",
    "    for ts in r.checkins:\n",
    "        rows.append({\"business_id\": r.business_id, \"checkin_time\": ts})\n",
    "events = pd.DataFrame(rows)\n",
    "\n",
    "# 4. Aggregate & filter\n",
    "agg = events.groupby(\"business_id\").checkin_time.count().rename(\"n_checkins\")\n",
    "# drop too‑few\n",
    "good = agg[agg >= 5].index\n",
    "events = events[events.business_id.isin(good)]\n",
    "\n",
    "# 5. Save clean set\n",
    "events.reset_index(drop=True).to_parquet(\"clean_checkins.parquet\")\n",
    "events.index += 1\n",
    "\n",
    "events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0724da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review.json\n",
    "# set up your output writer\n",
    "schema = None\n",
    "writer = None\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def parse_date(s):\n",
    "    try:\n",
    "        return datetime.strptime(s, \"%Y-%m-%d\")\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "def sentiment_ok(row):\n",
    "    s = analyzer.polarity_scores(row.text)[\"compound\"]\n",
    "    if row.stars >= 4 and s < 0:   return False\n",
    "    if row.stars <= 2 and s > 0.5: return False\n",
    "    return True\n",
    "\n",
    "chunksize = 100_000\n",
    "reader = pd.read_json(\"Data/review.json\", lines=True, chunksize=chunksize)\n",
    "\n",
    "for i, chunk in enumerate(reader):\n",
    "    # 1. mandatory columns + dedupe by review_id within chunk\n",
    "    mandatory = [\"review_id\",\"user_id\",\"business_id\",\"stars\",\"date\",\"text\"]\n",
    "    chunk = chunk.dropna(subset=mandatory)\n",
    "    chunk = chunk.drop_duplicates(\"review_id\", keep=\"first\")\n",
    "    \n",
    "    # 2. parse + date range\n",
    "    chunk[\"date_parsed\"] = chunk.date.map(parse_date)\n",
    "    mask = chunk.date_parsed.between(\"2010-01-01\", \"2020-12-31\")\n",
    "    chunk = chunk[mask]\n",
    "\n",
    "    # 3. stars bounds, text length\n",
    "    chunk = chunk[chunk.stars.between(1,5)]\n",
    "    chunk = chunk[chunk.text.str.len() >= 5]\n",
    "\n",
    "    # 4. hash‑based dedupe (within chunk)\n",
    "    chunk[\"text_hash\"] = chunk.text.map(lambda t: md5(t.encode(\"utf8\")).hexdigest())\n",
    "    chunk = chunk.drop_duplicates(\"text_hash\", keep=\"first\")\n",
    "\n",
    "    # 5. vote counts ≥0\n",
    "    for col in [\"useful\",\"funny\",\"cool\"]:\n",
    "        chunk = chunk[chunk[col] >= 0]\n",
    "\n",
    "    # 6. z‑score outliers — approximate by computing on chunk\n",
    "    for col in [\"useful\",\"funny\",\"cool\"]:\n",
    "        zs = zscore(chunk[col])\n",
    "        chunk = chunk[zs < 3]\n",
    "\n",
    "    # 7. sentiment mismatch\n",
    "    chunk = chunk[chunk.apply(sentiment_ok, axis=1)]\n",
    "\n",
    "    # 8. append to parquet\n",
    "    table = pa.Table.from_pandas(chunk.drop(columns=[\"text_hash\"]), preserve_index=False)\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(\"clean_reviews.parquet\", table.schema)\n",
    "    writer.write_table(table)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57697d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete! Saved to clean_tips.parquet with 806995 records.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>compliment_count</th>\n",
       "      <th>date_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGNUgVwnZUey3gcPCJ76iw</td>\n",
       "      <td>3uLgwr0qeCNMjKenHJwPGQ</td>\n",
       "      <td>Avengers time with the ladies.</td>\n",
       "      <td>2012-05-18 02:17:21</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-05-18 02:17:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NBN4MgHP9D3cw--SnauTkA</td>\n",
       "      <td>QoezRbYQncpRqyrLH6Iqjg</td>\n",
       "      <td>They have lots of good deserts and tasty cuban...</td>\n",
       "      <td>2013-02-05 18:35:10</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-02-05 18:35:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-copOvldyKh1qr-vzkDEvw</td>\n",
       "      <td>MYoRNLb5chwjQe3c_k37Gg</td>\n",
       "      <td>It's open even when you think it isn't</td>\n",
       "      <td>2013-08-18 00:56:08</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-08-18 00:56:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FjMQVZjSqY8syIO-53KFKw</td>\n",
       "      <td>hV-bABTK-glh5wj31ps_Jw</td>\n",
       "      <td>Very decent fried chicken</td>\n",
       "      <td>2017-06-27 23:05:38</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-06-27 23:05:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ld0AperBXk1h6UbqmM80zw</td>\n",
       "      <td>_uN0OudeJ3Zl_tf6nxg5ww</td>\n",
       "      <td>Appetizers.. platter special for lunch</td>\n",
       "      <td>2012-10-06 19:43:09</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-10-06 19:43:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id             business_id  \\\n",
       "1  AGNUgVwnZUey3gcPCJ76iw  3uLgwr0qeCNMjKenHJwPGQ   \n",
       "2  NBN4MgHP9D3cw--SnauTkA  QoezRbYQncpRqyrLH6Iqjg   \n",
       "3  -copOvldyKh1qr-vzkDEvw  MYoRNLb5chwjQe3c_k37Gg   \n",
       "4  FjMQVZjSqY8syIO-53KFKw  hV-bABTK-glh5wj31ps_Jw   \n",
       "5  ld0AperBXk1h6UbqmM80zw  _uN0OudeJ3Zl_tf6nxg5ww   \n",
       "\n",
       "                                                text                date  \\\n",
       "1                     Avengers time with the ladies. 2012-05-18 02:17:21   \n",
       "2  They have lots of good deserts and tasty cuban... 2013-02-05 18:35:10   \n",
       "3             It's open even when you think it isn't 2013-08-18 00:56:08   \n",
       "4                          Very decent fried chicken 2017-06-27 23:05:38   \n",
       "5             Appetizers.. platter special for lunch 2012-10-06 19:43:09   \n",
       "\n",
       "   compliment_count         date_parsed  \n",
       "1                 0 2012-05-18 02:17:21  \n",
       "2                 0 2013-02-05 18:35:10  \n",
       "3                 0 2013-08-18 00:56:08  \n",
       "4                 0 2017-06-27 23:05:38  \n",
       "5                 0 2012-10-06 19:43:09  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tip.json\n",
    "# Configuration\n",
    "INPUT_FILE = \"Data/tip.json\"\n",
    "OUTPUT_FILE = \"clean_tips.parquet\"\n",
    "DATE_MIN = \"2010-01-01\"\n",
    "DATE_MAX = \"2020-12-31\"\n",
    "TEXT_MIN_LEN = 5\n",
    "OUTLIER_Z = 3\n",
    "\n",
    "# 1. Load the full JSON into a DataFrame\n",
    "df = pd.read_json(INPUT_FILE, lines=True)\n",
    "\n",
    "# 2. Drop rows with missing mandatory fields\n",
    "mandatory = [\"text\", \"date\", \"compliment_count\", \"business_id\", \"user_id\"]\n",
    "df.dropna(subset=mandatory, inplace=True)\n",
    "\n",
    "# 3. Parse dates and filter by range\n",
    "df[\"date_parsed\"] = pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "mask_date = df[\"date_parsed\"].notna() & (\n",
    "    df[\"date_parsed\"] >= pd.to_datetime(DATE_MIN)\n",
    ") & (\n",
    "    df[\"date_parsed\"] <= pd.to_datetime(DATE_MAX)\n",
    ")\n",
    "df = df[mask_date]\n",
    "\n",
    "# 4. Enforce non-negative compliments and remove outliers\n",
    "df = df[df[\"compliment_count\"] >= 0]\n",
    "if not df.empty:\n",
    "    # compute z-scores and filter\n",
    "    z_scores = zscore(df[\"compliment_count\"].astype(float), nan_policy='omit')\n",
    "    mask_outlier = np.abs(z_scores) < OUTLIER_Z\n",
    "    df = df[mask_outlier]\n",
    "\n",
    "# 5. Filter by minimum text length\n",
    "df[\"text_len\"] = df[\"text\"].str.len().fillna(0)\n",
    "df = df[df[\"text_len\"] >= TEXT_MIN_LEN]\n",
    "\n",
    "# 6. De-duplicate by text hash\n",
    "df[\"text_hash\"] = df[\"text\"].apply(lambda t: md5(t.encode(\"utf-8\")).hexdigest())\n",
    "df.drop_duplicates(subset=[\"text_hash\"], inplace=True)\n",
    "\n",
    "# 7. Export cleaned tips\n",
    "df.drop(columns=[\"text_hash\", \"text_len\"], inplace=True)\n",
    "df.to_parquet(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"Cleaning complete! Saved to {OUTPUT_FILE} with {len(df)} records.\")\n",
    "\n",
    "df.index += 1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6b8c467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete! Saved to clean_users.parquet with 0 records.\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "INPUT_FILE = \"Data/user.json\"\n",
    "OUTPUT_FILE = \"clean_users.parquet\"\n",
    "DATE_MIN = \"2010-01-01\"\n",
    "DATE_MAX = \"2020-12-31\"\n",
    "OUTLIER_Z = 3\n",
    "CHUNK_SIZE = 100_000  # adjust based on available memory\n",
    "\n",
    "# Mandatory and numeric fields\n",
    "mandatory = [\n",
    "    \"user_id\", \"review_count\", \"yelping_since\",\n",
    "    \"useful\", \"funny\", \"cool\", \"fans\", \"average_stars\"\n",
    "]\n",
    "numeric_cols = [\n",
    "    \"review_count\", \"useful\", \"funny\", \"cool\", \"fans\", \"average_stars\",\n",
    "    \"compliment_hot\", \"compliment_more\", \"compliment_profile\", \"compliment_cute\",\n",
    "    \"compliment_list\", \"compliment_note\", \"compliment_plain\", \"compliment_cool\",\n",
    "    \"compliment_funny\", \"compliment_writer\", \"compliment_photos\"\n",
    "]\n",
    "\n",
    "# Helper: remove outliers by z-score\n",
    "def remove_outliers(df, cols, z=OUTLIER_Z):\n",
    "    for c in cols:\n",
    "        if c in df:\n",
    "            scores = zscore(df[c].astype(float), nan_policy='omit')\n",
    "            df = df[np.abs(scores) < z]\n",
    "    return df\n",
    "\n",
    "# Prepare Parquet writer and dedupe tracking\n",
    "writer = None\n",
    "seen_ids = set()\n",
    "\n",
    "# Process in chunks\n",
    "for chunk in pd.read_json(INPUT_FILE, lines=True, chunksize=CHUNK_SIZE):\n",
    "    # 1. Drop missing mandatory fields\n",
    "    chunk.dropna(subset=mandatory, inplace=True)\n",
    "\n",
    "    # 2. Ensure non-negative numeric values\n",
    "    for col in numeric_cols:\n",
    "        if col in chunk:\n",
    "            chunk = chunk[chunk[col] >= 0]\n",
    "\n",
    "    # 3. Parse dates and filter by range\n",
    "    chunk['joined'] = pd.to_datetime(chunk['yelping_since'], format=\"%Y-%m-%d\", errors='coerce')\n",
    "    mask = (\n",
    "        chunk['joined'].notna() &\n",
    "        (chunk['joined'] >= pd.to_datetime(DATE_MIN)) &\n",
    "        (chunk['joined'] <= pd.to_datetime(DATE_MAX))\n",
    "    )\n",
    "    chunk = chunk[mask]\n",
    "\n",
    "    # 4. Remove outliers\n",
    "    chunk = remove_outliers(chunk, numeric_cols)\n",
    "\n",
    "    # 5. Validate average_stars\n",
    "    chunk = chunk[(chunk['average_stars'] >= 1) & (chunk['average_stars'] <= 5)]\n",
    "\n",
    "    # 6. De-duplicate across chunks\n",
    "    mask_new = ~chunk['user_id'].isin(seen_ids)\n",
    "    chunk = chunk[mask_new]\n",
    "    seen_ids.update(chunk['user_id'].tolist())\n",
    "\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # 7. Drop and reorder columns\n",
    "    chunk.drop(columns=['yelping_since'], inplace=True)\n",
    "\n",
    "    # 8. Write to Parquet (append if writer exists)\n",
    "    table = pa.Table.from_pandas(chunk, preserve_index=False)\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(OUTPUT_FILE, table.schema)\n",
    "    writer.write_table(table)\n",
    "\n",
    "# Finalize\n",
    "if writer:\n",
    "    writer.close()\n",
    "print(f\"Cleaning complete! Saved to {OUTPUT_FILE} with {len(seen_ids)} records.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
